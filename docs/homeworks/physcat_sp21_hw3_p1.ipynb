{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "starting-evaluation",
   "metadata": {},
   "source": [
    "# HW3: Part 1 - JIT and Threading\n",
    "\n",
    "v1.0 (2021 Spring): Aled Cuda, Aditya Sengupta\n",
    "\n",
    "**Time Budget: 3 Hours**\n",
    "\n",
    "## [Just-In-Time Compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation):\n",
    "\n",
    "In order to transform Python code into machine instructions we use the library [Numba](https://numba.readthedocs.io/en/stable/user/index.html). The inner workings of Numba are devilishly complex, but broadly speaking, here's what happens when you call a function that has been marked for jit with Numba:\n",
    "\n",
    "1. When the Python interpereter tries to execute a function that you marked for JITing with a Numba decorator, it calls out to Numba.\n",
    "2. Numba checks its database to see if it has already compiled the given function. If it does, it verifies input types match those it was compiled with and calls that function. If not, it moves onto the following step.\n",
    "3. Numba will check the types of all the inputs, and use that information to figure out the types of every variable in the program as well as the return type. If it's successful at inferring the types, it proceeds onto the next step.\n",
    "4. Knowing the type of everything, Numba calls out to its compilation backend ([LLVM](https://llvm.org/docs/index.html)) which turns the Python code into machine code.\n",
    "\n",
    "As you can probably tell there are a number of steps in this chain that can fail. Chief among them is step 3. This is why JITing python is so difficult. Python is *SERIOUSLY* dynamically typed, which means it plays things fast and loose with the type of objects. One minute a function can be adding two integers, and the next it can be concatenating two strings, on the same line of code, using the same variables. Run the example below to see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "stringa = \"Hi\"\n",
    "stringb = \" Jeff\"\n",
    "\n",
    "a = 10\n",
    "b = 20\n",
    "\n",
    "print(\"dynamic_add(stringa, stringb):\", dynamic_add(stringa, stringb))\n",
    "print(\"type(dynamic_add(stringa, stringb)):\", type(dynamic_add(stringa, stringb)))\n",
    "\n",
    "print(\"dynamic_add(a, b):\", dynamic_add(a, b))\n",
    "print(\"type(dynamic_add(a, b)):\", type(dynamic_add(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-correction",
   "metadata": {},
   "source": [
    "As you can see, the same function operates on two completely different types of data. If you were to write this in assembly, you'd have to write two entirely different functions. It's almost definitely impossible to figure out statically (that is, without running the program) what type each variable will take on. By waiting until runtime, Numba mostly solves this problem, but it can't always figure out the types, so in a sense it only really supports a subset of Python.\n",
    "\n",
    "To JIT a function, you can use the `njit` decorator from Numba to tag it for compilation, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def numba_add(a, b):\n",
    "    return a + b\n",
    "\n",
    "%time numba_add(10.5, 19.5)\n",
    "%time numba_add(3.141, 3.141)\n",
    "%time numba_add(9.1, 8.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-murder",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "a. Run the cell above, explain why the first call to numba_add a couple thousand times longer than the second two\n",
    "\n",
    "**YOUR ANSWER**\n",
    "\n",
    "b. Run the cell below, it speed up above, so why did it slow down again before speeding back up?\n",
    "\n",
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time numba_add(\"Hi from \", \"Numba\")\n",
    "%time numba_add(\"Hi from \", \"Numba but faster this time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-closer",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "Modify the function below, which computes a sum of the squares of each element in an array, to run jited. Note the speedup you achieve:\n",
    "\n",
    "My speedup = xxx%\n",
    "\n",
    "(Don't worry this isn't a trick question, but don't use any numpy functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sum_squares(l):\n",
    "    running_sum = 0\n",
    "    for i in range(len(l)):\n",
    "        running_sum += l[i]**2\n",
    "    return running_sum\n",
    "\n",
    "# Trigger jit\n",
    "sum_squares(np.array([0.0, 1.0]))\n",
    "testvec = np.linspace(0, 100, 100000)\n",
    "\n",
    "%timeit sum_squares(testvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-ridge",
   "metadata": {},
   "source": [
    "Another way to parallelize Python code is via the Python `multiprocessing` library. Let's try that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "data = np.arange(25)\n",
    "\n",
    "def helper(x):\n",
    "    \"\"\"\n",
    "    Double and print a single element\n",
    "    \"\"\"\n",
    "    print(x*2)\n",
    "    return x*2\n",
    "\n",
    "def multi_doublenprint(a):\n",
    "    \"\"\"\n",
    "    Take an array as input, and use the map method from multiprocessings\n",
    "    Pool class to print out and return the double of each element\n",
    "    \"\"\"\n",
    "    p = Pool(16)\n",
    "    \n",
    "    # Use the Pool.map method to call our helper on each element of the array\n",
    "    return p.map(helper, a)\n",
    "\n",
    "multi_doublenprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-yacht",
   "metadata": {},
   "source": [
    "Huh???? In order to break the global interpereter lock, the multiprocessing library spawns multiple separate Python programs. It splits up the list we're operating on and sends them out to the different processes. It then combines the results from each and builds a new list out of them! This unfortunately causes the print output to get all mangled together. This is a good example of the non-deterministic nature of multithreading. Run this cell a couple of times, and see how the output changes.\n",
    "\n",
    "The other way to write decent parallel code is to explicitly parallelize a loop with Numba. This has the advantage of being both more efficient and more compact than the multiprocessing way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import prange\n",
    "\n",
    "# We need to use parallel=True, otherwise prange just acts like range\n",
    "@njit(parallel = True)\n",
    "def explicit_doublenprint(a):\n",
    "    # Create an array to hold our result\n",
    "    ret = np.empty(a.size, dtype = np.int64)\n",
    "    # Use the prange function from numba to dispatch different iterations of our for loop on different threads\n",
    "    for i in prange(0, len(a)):\n",
    "        print(2*a[i])\n",
    "        ret[i] = 2*a[i]\n",
    "    return ret\n",
    "\n",
    "explicit_doublenprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-terrorism",
   "metadata": {},
   "source": [
    "You'll notice that the print output is quite a bit less garbled. This is because the way Numba spawns threads is much lighter weight and quite a bit saner. This is because for the most part, it can just ignore the global interpreter lock and run multiple concurrent threads in the same Python process.\n",
    "\n",
    "The last, easiest, and generally fastest way to run Python code in parallel with Numba is to use the vectorize decorator and to set the target to parallel. For some reason, this also requires you to explicitly specify the input types, as you'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "# We ask numba to generate parallel code that takes an int64 as an\n",
    "# argument (the part in parenthesis) and returns an int64, int64 is\n",
    "# the default numpy integer datatype, and float64 is the default\n",
    "# floating point datatype for numpy\n",
    "@vectorize('int64(int64)', target='parallel')\n",
    "def vectored_doublenprint(x):\n",
    "    print(x * 2)\n",
    "    return x * 2\n",
    "\n",
    "vectored_doublenprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-ceramic",
   "metadata": {},
   "source": [
    "As you can see this operates in essentially the same way as the prange example, but the expression is much clearer, and as we probably saw in lecture, Numba gets a little bit smarter with the vectorization and this way is typicaly a little faster.\n",
    "\n",
    "For the rest of this part of the homework, we'll explore ways of speeding up calculations of the Fibonacci sequence, defined in plain Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "neural-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    lastnum = 1\n",
    "    lastlastnum = 0\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "    i = 2\n",
    "    while i < n:\n",
    "        i += 1\n",
    "        lastlastnum, lastnum = lastnum, lastnum + lastlastnum\n",
    "    return lastnum + lastlastnum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-ensemble",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "Use `multiprocessing.Pool` to compute `fibonacci(k)` for each element `k` in an array, where `fibonacci(k)` is the kth element of the Fibonacci sequence as given by the function above. Feel free to use the given Fibonacci code in your test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "surface-presence",
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d7d0023da151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfibonacci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfibonacci_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def fibonacci_pool(arr):\n",
    "    \"\"\"\n",
    "    Fill in this function to use the multiprocessing library to compute the fibonacci\n",
    "    sequence for every element in the array, and return the result as an array\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "assert np.all(np.array([fibonacci(i) for i in range(100)]) == fibonacci_pool(np.arange(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-reconstruction",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "\n",
    "Use Numba's `prange` object and the `@njit(parallel=True)` decorator to explicitly parallelize this function.\n",
    "\n",
    "If you decide to call out to the fibonacci function, make a copy of it under a different name in this cell and decorate it with @njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to decorate this funcition for jiting and enable parallelism\n",
    "def fibonacci_prange(arr):\n",
    "    \"\"\"\n",
    "    Fill in this function with an explicit loop over the array using numba's prange function\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "assert np.all(np.array([fibonacci(i) for i in range(100)]) == fibonacci_prange(np.arange(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-insertion",
   "metadata": {},
   "source": [
    "### Question 5:\n",
    "\n",
    "Use Numba's `@vectorize` decorator to implicitly parallelize the Fibonacci program. You will probably want the type declaration `'int64(int64)` and the compilation target `target='parallel'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to decorate this function with the vectorize decorator, and target='parallel'\n",
    "def fibonacci_vectorize(x):\n",
    "    \"\"\"\n",
    "    Fill in this function to compute the xth element of the fibonacci sequence and\n",
    "    let numba handle the parallelism\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "assert np.all(np.array([fibonacci(i) for i in range(100)]) == fibonacci_vectorize(np.arange(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-stability",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "\n",
    "Run the cell below, remark on the performance differences between each of the implementations, do any of these perform better/worse than you were expecting?\n",
    "\n",
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "testvec = np.random.randint(0, 300, size=100000)\n",
    "\n",
    "print(\"Naive python implementation:\")\n",
    "%timeit _ = np.array([fibonacci(i) for i in testvec])\n",
    "print(\"Python multiprocessing pool implementation:\")\n",
    "%timeit _ = fibonacci_pool(testvec)\n",
    "print(\"Numba explicitly parallel implementation:\")\n",
    "%timeit _ = fibonacci_prange(testvec)\n",
    "print(\"Numba vectorized implementation:\")\n",
    "%timeit _ = fibonacci_vectorize(testvec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}